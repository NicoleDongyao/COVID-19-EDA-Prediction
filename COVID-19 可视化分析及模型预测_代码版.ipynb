{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COVID-19 可视化分析及模型预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Time: 2021/01/09**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "**[1.全球疫情速览：是谁为人民服务，是谁一地鸡毛](#id_overview)**<br/>\n",
    "**[2.COVID-19相关文献的文本研究：目光所言何处](#id_literature)**<br/>\n",
    "**[3.未来确诊人数发展预测：黎明前的黑暗](#id_prediction)**<br/>\n",
    "**[4.医药统计相关职位分析：数据科学家的新赛道](#id_medicine)**<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import sys\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# --- plotly ---\n",
    "from plotly import tools, subplots\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.io as pio\n",
    "pio.templates.default = \"plotly_white\"\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# --- models ---\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "\n",
    "# --- setup ---\n",
    "pd.set_option('max_columns', 50)\n",
    "\n",
    "# --- wordcloud ---\n",
    "import os\n",
    "from os import path\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud,STOPWORDS,ImageColorGenerator\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from imageio import imread\n",
    "import random\n",
    "\n",
    "# --- pyecharts ---\n",
    "import pandas as pd\n",
    "from pyecharts.charts import Pie\n",
    "from pyecharts import options as opts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pyecharts\n",
    "#pip install pillow wordcloud imageio jieba snownlp itchat -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "#pip install imageio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"id_overview\"></a>\n",
    "# 1. 全球疫情时空数据可视化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Globel ---\n",
    "from datetime import datetime\n",
    "def _convert_date_str(df):\n",
    "    try:\n",
    "        df.columns = list(df.columns[:4]) + [datetime.strptime(d, \"%m/%d/%y\").date().strftime(\"%Y-%m-%d\") for d in df.columns[4:]]\n",
    "    except:\n",
    "        print('_convert_date_str failed with %y, try %Y')\n",
    "        df.columns = list(df.columns[:4]) + [datetime.strptime(d, \"%m/%d/%Y\").date().strftime(\"%Y-%m-%d\") for d in df.columns[4:]]\n",
    "confirmed_global_df = pd.read_csv('time_series_covid19_confirmed_global.csv')\n",
    "_convert_date_str(confirmed_global_df)\n",
    "\n",
    "deaths_global_df = pd.read_csv('time_series_covid19_deaths_global.csv')\n",
    "_convert_date_str(deaths_global_df)\n",
    "\n",
    "recovered_global_df = pd.read_csv('time_series_covid19_recovered_global.csv')\n",
    "_convert_date_str(recovered_global_df)\n",
    "\n",
    "# Filter out problematic data points (The West Bank and Gaza had a negative value, cruise ships were associated with Canada, etc.)\n",
    "removed_states = \"Recovered|Grand Princess|Diamond Princess\"\n",
    "removed_countries = \"US|The West Bank and Gaza\"\n",
    "\n",
    "confirmed_global_df.rename(columns={\"Province/State\": \"Province_State\", \"Country/Region\": \"Country_Region\"}, inplace=True)\n",
    "deaths_global_df.rename(columns={\"Province/State\": \"Province_State\", \"Country/Region\": \"Country_Region\"}, inplace=True)\n",
    "recovered_global_df.rename(columns={\"Province/State\": \"Province_State\", \"Country/Region\": \"Country_Region\"}, inplace=True)\n",
    "\n",
    "confirmed_global_df = confirmed_global_df[~confirmed_global_df[\"Province_State\"].replace(np.nan, \"nan\").str.match(removed_states)]\n",
    "deaths_global_df    = deaths_global_df[~deaths_global_df[\"Province_State\"].replace(np.nan, \"nan\").str.match(removed_states)]\n",
    "recovered_global_df = recovered_global_df[~recovered_global_df[\"Province_State\"].replace(np.nan, \"nan\").str.match(removed_states)]\n",
    "\n",
    "confirmed_global_df = confirmed_global_df[~confirmed_global_df[\"Country_Region\"].replace(np.nan, \"nan\").str.match(removed_countries)]\n",
    "deaths_global_df    = deaths_global_df[~deaths_global_df[\"Country_Region\"].replace(np.nan, \"nan\").str.match(removed_countries)]\n",
    "recovered_global_df = recovered_global_df[~recovered_global_df[\"Country_Region\"].replace(np.nan, \"nan\").str.match(removed_countries)]\n",
    "\n",
    "confirmed_global_melt_df = confirmed_global_df.melt(\n",
    "    id_vars=['Country_Region', 'Province_State', 'Lat', 'Long'], value_vars=confirmed_global_df.columns[4:], var_name='Date', value_name='ConfirmedCases')\n",
    "deaths_global_melt_df = deaths_global_df.melt(\n",
    "    id_vars=['Country_Region', 'Province_State', 'Lat', 'Long'], value_vars=confirmed_global_df.columns[4:], var_name='Date', value_name='Deaths')\n",
    "recovered_global_melt_df = deaths_global_df.melt(\n",
    "    id_vars=['Country_Region', 'Province_State', 'Lat', 'Long'], value_vars=confirmed_global_df.columns[4:], var_name='Date', value_name='Recovered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = confirmed_global_melt_df.merge(deaths_global_melt_df, on=['Country_Region', 'Province_State', 'Lat', 'Long', 'Date'])\n",
    "train = train.merge(recovered_global_melt_df, on=['Country_Region', 'Province_State', 'Lat', 'Long', 'Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- US ---\n",
    "confirmed_us_df = pd.read_csv('time_series_covid19_confirmed_US.csv')\n",
    "deaths_us_df = pd.read_csv('time_series_covid19_deaths_US.csv')\n",
    "\n",
    "confirmed_us_df.drop(['UID', 'iso2', 'iso3', 'code3', 'FIPS', 'Admin2', 'Combined_Key'], inplace=True, axis=1)\n",
    "deaths_us_df.drop(['UID', 'iso2', 'iso3', 'code3', 'FIPS', 'Admin2', 'Combined_Key', 'Population'], inplace=True, axis=1)\n",
    "\n",
    "confirmed_us_df.rename({'Long_': 'Long'}, axis=1, inplace=True)\n",
    "deaths_us_df.rename({'Long_': 'Long'}, axis=1, inplace=True)\n",
    "\n",
    "_convert_date_str(confirmed_us_df)\n",
    "_convert_date_str(deaths_us_df)\n",
    "\n",
    "# clean\n",
    "confirmed_us_df = confirmed_us_df[~confirmed_us_df.Province_State.str.match(\"Diamond Princess|Grand Princess|Recovered|Northern Mariana Islands|American Samoa\")]\n",
    "deaths_us_df = deaths_us_df[~deaths_us_df.Province_State.str.match(\"Diamond Princess|Grand Princess|Recovered|Northern Mariana Islands|American Samoa\")]\n",
    "\n",
    "# --- Aggregate by province state ---\n",
    "#confirmed_us_df.groupby(['Country_Region', 'Province_State'])\n",
    "confirmed_us_df = confirmed_us_df.groupby(['Country_Region', 'Province_State']).sum().reset_index()\n",
    "deaths_us_df = deaths_us_df.groupby(['Country_Region', 'Province_State']).sum().reset_index()\n",
    "\n",
    "# remove lat, long.\n",
    "confirmed_us_df.drop(['Lat', 'Long'], inplace=True, axis=1)\n",
    "deaths_us_df.drop(['Lat', 'Long'], inplace=True, axis=1)\n",
    "\n",
    "confirmed_us_melt_df = confirmed_us_df.melt(\n",
    "    id_vars=['Country_Region', 'Province_State'], value_vars=confirmed_us_df.columns[2:], var_name='Date', value_name='ConfirmedCases')\n",
    "deaths_us_melt_df = deaths_us_df.melt(\n",
    "    id_vars=['Country_Region', 'Province_State'], value_vars=deaths_us_df.columns[2:], var_name='Date', value_name='Deaths')\n",
    "\n",
    "train_us = confirmed_us_melt_df.merge(deaths_us_melt_df, on=['Country_Region', 'Province_State', 'Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globel与US合并\n",
    "train = pd.concat([train, train_us], axis=0, sort=False)\n",
    "\n",
    "train_us.rename({'Country_Region': 'country', 'Province_State': 'province', 'Date': 'date', 'ConfirmedCases': 'confirmed', 'Deaths': 'fatalities'}, axis=1, inplace=True)\n",
    "train_us['country_province'] = train_us['country'].fillna('') + '/' + train_us['province'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.rename({'Country_Region': 'country', 'Province_State': 'province', 'Date': 'date', 'ConfirmedCases': 'confirmed', 'Deaths': 'fatalities', 'Recovered': 'recovered'}, axis=1, inplace=True)\n",
    "train['country_province'] = train['country'].fillna('') + '/' + train['province'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 全球变化趋势/各个国家变化趋势"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 全球变化趋势"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 疫情首先出现在中国，并在很短的时间内便席卷了亚洲。进入二月份以后，尽管打着”反中抗中“旗号赚取选票的\n",
    "# 政客持续反对，但当欧美国家的核酸检测在CDC的强烈推动下较大面积铺开后，这些国家（以美国为代表）\n",
    "# 的确诊人数数据急剧攀升，并最终演化为今天的疫情重灾区。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_df = train.groupby(['date', 'country'])[['confirmed', 'fatalities']].sum().reset_index()\n",
    "countries = country_df['country'].unique()\n",
    "target_date = country_df['date'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_df['date'] = country_df['date'].apply(str)\n",
    "country_df['confirmed'] = country_df['confirmed']\n",
    "country_df['fatalities'] = country_df['fatalities']\n",
    "\n",
    "fig = px.scatter_geo(country_df, locations=\"country\", locationmode='country names', \n",
    "                     color=\"confirmed\", size='confirmed', hover_name=\"country\", \n",
    "                     hover_data=['confirmed', 'fatalities'],\n",
    "                     range_color= [0, country_df['confirmed'].max()], \n",
    "                     projection=\"natural earth\", animation_frame=\"date\", \n",
    "                     title='COVID-19: Confirmed cases spread Over Time', color_continuous_scale=\"portland\")\n",
    "# fig.update(layout_coloraxis_showscale=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_geo(country_df, locations=\"country\", locationmode='country names', \n",
    "                     color=\"fatalities\", size='fatalities', hover_name=\"country\", \n",
    "                     hover_data=['confirmed', 'fatalities'],\n",
    "                     range_color= [0, country_df['fatalities'].max()], \n",
    "                     projection=\"natural earth\", animation_frame=\"date\", \n",
    "                     title='COVID-19: Fatalities growth Over Time', color_continuous_scale=\"portland\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_df['prev_confirmed'] = country_df.groupby('country')['confirmed'].shift(1)\n",
    "country_df['new_case'] = country_df['confirmed'] - country_df['prev_confirmed']\n",
    "country_df['new_case'].fillna(0, inplace=True)\n",
    "\n",
    "country_df.loc[country_df['new_case'] < 0, 'new_case'] = 0.\n",
    "fig = px.scatter_geo(country_df, locations=\"country\", locationmode='country names', \n",
    "                     color=\"new_case\", size='new_case', hover_name=\"country\", \n",
    "                     hover_data=['confirmed', 'fatalities'],\n",
    "                     range_color= [0, country_df['new_case'].max()], \n",
    "                     projection=\"natural earth\", animation_frame=\"date\", \n",
    "                     title='COVID-19: Daily NEW cases over Time', color_continuous_scale=\"portland\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ww_df = train.groupby('date')[['confirmed', 'fatalities']].sum().reset_index()\n",
    "ww_df['new_case'] = ww_df['confirmed'] - ww_df['confirmed'].shift(1)\n",
    "ww_df['growth_factor'] = ww_df['new_case'] / ww_df['new_case'].shift(1)\n",
    "\n",
    "ww_df['mortality'] = round(ww_df['fatalities'] / ww_df['confirmed'], 4)\n",
    "\n",
    "fig = px.line(ww_df, x=\"date\", y=\"mortality\", \n",
    "              title=\"Worldwide Mortality Rate Over Time\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 二月份的死亡率快速下降得益于以中国大陆为首的亚洲疫情重灾区国家相对严格的封锁政策、减少人员流动\n",
    "## 然而，三月份开始，当亚洲以外的国家大量进行检测时，成倍的死亡被追踪到为新冠病毒。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(ww_df, x=\"date\", y=\"growth_factor\", \n",
    "              title=\"Worldwide Growth Factor Over Time\")\n",
    "fig.add_trace(go.Scatter(x=[ww_df['date'].min(), ww_df['date'].max()], y=[1., 1.], name='Growth factor=1.', line=dict(dash='dash', color=('rgb(255, 0, 0)'))))\n",
    "fig.update_yaxes(range=[0., 5.])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE：\n",
    "#Growth factor怎么得到的？\n",
    "#Boris Johnson: 3.16宣布自然免疫策略；4月8日确诊；Trump：10月12日确诊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Growth factor由每日增量与前一日存量做比率得到。五月份以前的剧烈波动源于之前提到过的严格的封城与欧美国家大量检测导致的增长因子大幅波动。\n",
    "#而另一方面，五月至十一月的增长率维持在1左右。相应时间段的夏季高温与普遍封城是控制疫情成功的主要原因。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 各国家变化趋势"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_df = train.groupby(['date', 'country'])[['confirmed', 'fatalities']].sum().reset_index()\n",
    "country_df.tail()\n",
    "\n",
    "countries = country_df['country'].unique()\n",
    "#print(f'{len(countries)} countries are in dataset:\\n{countries}')\n",
    "\n",
    "target_date = country_df['date'].max()\n",
    "\n",
    "print('Date: ', target_date)\n",
    "for i in [1, 100, 10000, 100000, 1000000, 10000000]:\n",
    "    n_countries = len(country_df.query('(date == @target_date) & confirmed > @i'))\n",
    "    print(f'{n_countries} countries have more than {i} confirmed cases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_country_df = country_df.query('date == @target_date')\n",
    "all_country_df['confirmed_log1p'] = np.log10(all_country_df['confirmed'] + 1)\n",
    "all_country_df['fatalities_log1p'] = np.log10(all_country_df['fatalities'] + 1)\n",
    "all_country_df['mortality_rate'] = all_country_df['fatalities'] / all_country_df['confirmed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.choropleth(all_country_df, locations=\"country\", \n",
    "                    locationmode='country names', color=\"confirmed_log1p\", \n",
    "                    hover_name=\"country\", hover_data=[\"confirmed\", 'fatalities', 'mortality_rate'],\n",
    "                    range_color=[all_country_df['confirmed_log1p'].min(), all_country_df['confirmed_log1p'].max()], \n",
    "                    color_continuous_scale=\"peach\", \n",
    "                    title='Countries with Confirmed Cases')\n",
    "\n",
    "# I'd like to update colorbar to show raw values, but this does not work somehow...\n",
    "# Please let me know if you know how to do this!!\n",
    "trace1 = list(fig.select_traces())[0]\n",
    "trace1.colorbar = go.choropleth.ColorBar(\n",
    "    tickvals=[0, 1, 2, 3, 4, 5, 6, 7, 8],\n",
    "    ticktext=['1', '10', '100', '1000','10000', '100000', '1000000', '10000000', '100000000'])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.choropleth(all_country_df, locations=\"country\", \n",
    "                    locationmode='country names', color=\"mortality_rate\", \n",
    "                    hover_name=\"country\", range_color=[0, 0.10], \n",
    "                    color_continuous_scale=\"peach\", \n",
    "                    title='Countries with mortality rate')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_latest = country_df.query('date == @target_date')\n",
    "\n",
    "fig = px.treemap(country_latest, path=[\"country\"], values='confirmed', height=700,\n",
    "                 title='Confirmed', color_discrete_sequence = px.colors.qualitative.Dark2)\n",
    "fig.data[0].textinfo = 'label+text+value'\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这两张树状图清晰地展现了以美国英国为代表的西方发达国家、以印度巴西为代表的医疗设施落后\n",
    "# 的发展中国家在确诊人数和死亡率上不容乐观的表现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = px.treemap(country_latest, path=[\"country\"], values='fatalities', height=700,\n",
    "                 title='Fatalities', color_discrete_sequence = px.colors.qualitative.Dark2)\n",
    "fig.data[0].textinfo = 'label+text+value'\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.distplot(np.log10(country_df.query('date == \"2021-01-07\"')['confirmed'] + 1))\n",
    "ax.set_title(\"confirmed cases histogram on 2021/1/7\")\n",
    "ax.set_xlim([0, 8])\n",
    "ax.set_xticks(np.arange(9))\n",
    "_ = ax.set_xticklabels(['0', '10', '100', '1k', '10k', '100k', '1m', '10m'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "top_country_df = country_df.query('(date == @target_date) & (confirmed > 100)')\n",
    "top_country_df['mortality_rate'] = round(top_country_df['fatalities'] / top_country_df['confirmed'], 4)\n",
    "top_country_df = top_country_df.sort_values('mortality_rate', ascending=False)\n",
    "\n",
    "fig = px.bar(top_country_df[:30].iloc[::-1],\n",
    "             x='mortality_rate', y='country',\n",
    "             title=f'Mortality rate HIGH: top 30 countries on {target_date}', text='mortality_rate', height=800, orientation='h')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_countries = 20\n",
    "n_start_death = 10\n",
    "fatality_top_countires = top_country_df.sort_values('fatalities', ascending=False).iloc[:n_countries]['country'].values\n",
    "country_df['date'] = pd.to_datetime(country_df['date'])\n",
    "\n",
    "\n",
    "df_list = []\n",
    "for country in fatality_top_countires:\n",
    "    this_country_df = country_df.query('country == @country')\n",
    "    start_date = this_country_df.query('fatalities > @n_start_death')['date'].min()\n",
    "    this_country_df = this_country_df.query('date >= @start_date')\n",
    "    this_country_df['date_since'] = this_country_df['date'] - start_date\n",
    "    this_country_df['fatalities_log1p'] = np.log10(this_country_df['fatalities'] + 1)\n",
    "    this_country_df['fatalities_log1p'] -= this_country_df['fatalities_log1p'].values[0]\n",
    "    df_list.append(this_country_df)\n",
    "\n",
    "tmpdf = pd.concat(df_list)\n",
    "tmpdf['date_since_days'] = tmpdf['date_since'] / pd.Timedelta('1 days')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top30_countries = top_country_df.sort_values('confirmed', ascending=False).iloc[:30]['country'].unique()\n",
    "country_df['prev_confirmed'] = country_df.groupby('country')['confirmed'].shift(1)\n",
    "country_df['new_case'] = country_df['confirmed'] - country_df['prev_confirmed']\n",
    "country_df['new_case'].fillna(0, inplace=True)\n",
    "top30_country_df = country_df[country_df['country'].isin(top30_countries)]\n",
    "\n",
    "fig = px.line(top30_country_df,\n",
    "              x='date', y='new_case', color='country',\n",
    "              title=f'DAILY NEW Confirmed cases by country')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_df['avg_new_case'] = country_df.groupby('country')['new_case'].rolling(7).mean().reset_index(0, drop=True)\n",
    "country_df['prev_new_case'] = country_df.groupby('country')['avg_new_case'].shift(1)\n",
    "country_df['growth_factor'] = country_df['avg_new_case'] / country_df['prev_new_case']\n",
    "\n",
    "country_df['growth_factor'].fillna(0, inplace=True)\n",
    "top30_country_df = country_df[country_df['country'].isin(top30_countries)]\n",
    "\n",
    "fig = px.line(top30_country_df,\n",
    "              x='date', y='growth_factor', color='country',\n",
    "              title=f'Growth factor by country')\n",
    "fig.add_trace(go.Scatter(x=[ww_df['date'].min(), ww_df['date'].max()], y=[1., 1.], name='Growth factor=1.', line=dict(dash='dash', color=('rgb(255, 0, 0)'))))\n",
    "fig.update_yaxes(range=[0., 5.])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 重点地区与重要时段"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1  美国"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for country in countries:\n",
    "    province = train.query('country == @country')['province'].unique()\n",
    "    if len(province) > 1:       \n",
    "        print(f'Country {country} has {len(province)} provinces: {province}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_state_code_df = pd.read_csv('usa_states2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data frame only for US. \n",
    "\n",
    "#train_us = train.query('country == \"US\"')\n",
    "train_us['mortality_rate'] = train_us['fatalities'] / train_us['confirmed']\n",
    "\n",
    "# Convert province column to its 2-char code name,\n",
    "state_name_to_code = dict(zip(usa_state_code_df['state_name'], usa_state_code_df['state_code']))\n",
    "train_us['province_code'] = train_us['province'].map(state_name_to_code)\n",
    "\n",
    "# Only show latest days.\n",
    "train_us_latest = train_us.query('date == @target_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.choropleth(train_us_latest, locations='province_code', locationmode=\"USA-states\",\n",
    "                    color='confirmed', scope=\"usa\", hover_data=['province', 'fatalities', 'mortality_rate'],\n",
    "                    title=f'Confirmed cases in US on {target_date}')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_us_latest.sort_values('confirmed', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.choropleth(train_us_latest, locations='province_code', locationmode=\"USA-states\",\n",
    "                    color='mortality_rate', scope=\"usa\", hover_data=['province', 'fatalities', 'mortality_rate'],\n",
    "                    title=f'Mortality rate in US on {target_date}')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_us_march = train_us.query('date > \"2020-03-01\"')\n",
    "fig = px.line(train_us_march,\n",
    "              x='date', y='confirmed', color='province',\n",
    "              title=f'Confirmed cases by state in US, as of {target_date}')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_us_march['prev_confirmed'] = train_us_march.groupby('province')['confirmed'].shift(1)\n",
    "train_us_march['new_case'] = train_us_march['confirmed'] - train_us_march['prev_confirmed']\n",
    "train_us_march['new_case'].fillna(0, inplace=True)\n",
    "\n",
    "fig = px.line(train_us_march,\n",
    "              x='date', y='new_case', color='province',\n",
    "              title=f'DAILY NEW Confirmed cases by states in US')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_us_march['avg_new_case'] = train_us_march.groupby('province')['new_case'].rolling(7).mean().reset_index(0, drop=True)\n",
    "train_us_march['prev_new_case'] = train_us_march.groupby('province')['avg_new_case'].shift(1)\n",
    "train_us_march['growth_factor'] = train_us_march['avg_new_case'] / train_us_march['prev_new_case']\n",
    "train_us_march['growth_factor'].fillna(0, inplace=True)\n",
    "fig = px.line(train_us_march,\n",
    "              x='date', y='growth_factor', color='province',\n",
    "              title=f'Growth factor by state in US')\n",
    "fig.add_trace(go.Scatter(x=[train_us_march['date'].min(), train_us_march['date'].max()], y=[1., 1.],\n",
    "                         name='Growth factor=1.', line=dict(dash='dash', color=('rgb(255, 0, 0)'))))\n",
    "fig.update_yaxes(range=[0., 5.])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2  欧洲"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref: https://www.kaggle.com/abhinand05/covid-19-digging-a-bit-deeper\n",
    "europe_country_list =list([\n",
    "    'Austria','Belgium','Bulgaria','Croatia','Cyprus','Czechia','Denmark','Estonia','Finland','France','Germany','Greece','Hungary','Ireland',\n",
    "    'Italy', 'Latvia','Luxembourg','Lithuania','Malta','Norway','Netherlands','Poland','Portugal','Romania','Slovakia','Slovenia',\n",
    "    'Spain', 'Sweden', 'United Kingdom', 'Iceland', 'Russia', 'Switzerland', 'Serbia', 'Ukraine', 'Belarus',\n",
    "    'Albania', 'Bosnia and Herzegovina', 'Kosovo', 'Moldova', 'Montenegro', 'North Macedonia'])\n",
    "\n",
    "country_df['date'] = pd.to_datetime(country_df['date'])\n",
    "train_europe = country_df[country_df['country'].isin(europe_country_list)]\n",
    "#train_europe['date_str'] = pd.to_datetime(train_europe['date'])\n",
    "train_europe_latest = train_europe.query('date == @target_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.choropleth(train_europe_latest, locations=\"country\", \n",
    "                    locationmode='country names', color=\"confirmed\", \n",
    "                    hover_name=\"country\", range_color=[1, train_europe_latest['confirmed'].max()], \n",
    "                    color_continuous_scale='portland', \n",
    "                    title=f'European Countries with Confirmed Cases as of {target_date}', scope='europe', height=800)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_europe_march = train_europe.query('date >= \"2020-03-01\"')\n",
    "fig = px.line(train_europe_march,\n",
    "              x='date', y='confirmed', color='country',\n",
    "              title=f'Confirmed cases by country in Europe, as of {target_date}')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(train_europe_march,\n",
    "              x='date', y='fatalities', color='country',\n",
    "              title=f'Fatalities by country in Europe, as of {target_date}')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_europe_march['prev_confirmed'] = train_europe_march.groupby('country')['confirmed'].shift(1)\n",
    "train_europe_march['new_case'] = train_europe_march['confirmed'] - train_europe_march['prev_confirmed']\n",
    "fig = px.line(train_europe_march,\n",
    "              x='date', y='new_case', color='country',\n",
    "              title=f'DAILY NEW Confirmed cases by country in Europe')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_europe_march['avg_new_case'] = train_europe_march.groupby('country')['new_case'].rolling(7).mean().reset_index(0, drop=True)\n",
    "train_europe_march['prev_new_case'] = train_europe_march.groupby('country')['avg_new_case'].shift(1)\n",
    "train_europe_march['growth_factor'] = train_europe_march['avg_new_case'] / train_europe_march['prev_new_case']\n",
    "train_europe_march['growth_factor'].fillna(0, inplace=True)\n",
    "fig = px.line(train_europe_march,\n",
    "              x='date', y='growth_factor', color='country',\n",
    "              title=f'Growth factor by country in Europe')\n",
    "fig.add_trace(go.Scatter(x=[train_europe_march['date'].min(), train_europe_march['date'].max()], y=[1., 1.],\n",
    "                         name='Growth factor=1.', line=dict(dash='dash', color=('rgb(255, 0, 0)'))))\n",
    "fig.update_yaxes(range=[0., 5.])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3  亚洲"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_latest = country_df.query('date == @target_date')\n",
    "\n",
    "fig = px.choropleth(country_latest, locations=\"country\", \n",
    "                    locationmode='country names', color=\"confirmed\", \n",
    "                    hover_name=\"country\", range_color=[1, 300000], \n",
    "                    color_continuous_scale='portland', \n",
    "                    title=f'Asian Countries with Confirmed Cases as of {target_date}', scope='asia', height=800)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_asian_country_df = country_df[country_df['country'].isin([\n",
    "    'China', 'Indonesia', 'Iran', 'Japan', 'Korea, South', 'Malaysia', 'Philippines',\n",
    "    'India', 'Bangladesh', 'Pakistan', 'Saudi Arabia', 'Turkey'\n",
    "])]\n",
    "\n",
    "fig = px.line(top_asian_country_df,\n",
    "              x='date', y='new_case', color='country',\n",
    "              title=f'DAILY NEW Confirmed cases Asia')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_asian_country_df['avg_new_case'] = top_asian_country_df.groupby('country')['new_case'].rolling(7).mean().reset_index(0, drop=True)\n",
    "top_asian_country_df['prev_new_case'] = top_asian_country_df.groupby('country')['avg_new_case'].shift(1)\n",
    "top_asian_country_df['growth_factor'] = top_asian_country_df['avg_new_case'] / top_asian_country_df['prev_new_case']\n",
    "top_asian_country_df['growth_factor'].fillna(0, inplace=True)\n",
    "fig = px.line(top_asian_country_df,\n",
    "              x='date', y='growth_factor', color='country',\n",
    "              title=f'Growth factor by country in Asia')\n",
    "fig.add_trace(go.Scatter(x=[top_asian_country_df['date'].min(), top_asian_country_df['date'].max()], y=[1., 1.],\n",
    "                         name='Growth factor=1.', line=dict(dash='dash', color=('rgb(255, 0, 0)'))))\n",
    "fig.update_yaxes(range=[0., 5.])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 正在恢复的国家"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "china_df = train.query('country == \"China\"')\n",
    "china_df['prev_confirmed'] = china_df.groupby('province')['confirmed'].shift(1)\n",
    "china_df['new_case'] = china_df['confirmed'] - china_df['prev_confirmed']\n",
    "china_df.loc[china_df['new_case'] < 0, 'new_case'] = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(china_df,\n",
    "              x='date', y='new_case', color='province',\n",
    "              title=f'DAILY NEW Confirmed cases in China by province')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"id_literature\"></a>\n",
    "# 2. COVID-19相关文献的文本研究"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**章节说明：**在文献分析这一部分，分析主要围绕文献的特点、词频等展开，考察了文献的字数分布、文献摘要出现的高频词、文献中国家被提及的次数、文献中其它常见疾病被提及的次数，和文献引用的参考文献。通过这一部分的分析，可以对于新冠肺炎的相关文献有着一个初步的了解。  \n",
    "**数据来源：**文献分析部分的数据来源于Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "json文件的形式如下，基本是字典嵌套字典的形式，涉及文献的id、标题、作者、摘要、分段的正文和参考文献等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON schema of full text documents\n",
    "\n",
    "\n",
    "{\n",
    "    \"paper_id\": <str>,                      # 40-character sha1 of the PDF\n",
    "    \"metadata\": {\n",
    "        \"title\": <str>,\n",
    "        \"authors\": [                        # list of author dicts, in order\n",
    "            {\n",
    "                \"first\": <str>,\n",
    "                \"middle\": <list of str>,\n",
    "                \"last\": <str>,\n",
    "                \"suffix\": <str>,\n",
    "                \"affiliation\": <dict>,\n",
    "                \"email\": <str>\n",
    "            },\n",
    "            ...\n",
    "        ],\n",
    "        \"abstract\": [                       # list of paragraphs in the abstract\n",
    "            {\n",
    "                \"text\": <str>,\n",
    "                \"cite_spans\": [             # list of character indices of inline citations\n",
    "                                            # e.g. citation \"[7]\" occurs at positions 151-154 in \"text\"\n",
    "                                            #      linked to bibliography entry BIBREF3\n",
    "                    {\n",
    "                        \"start\": 151,\n",
    "                        \"end\": 154,\n",
    "                        \"text\": \"[7]\",\n",
    "                        \"ref_id\": \"BIBREF3\"\n",
    "                    },\n",
    "                    ...\n",
    "                ],\n",
    "                \"ref_spans\": <list of dicts similar to cite_spans>,     # e.g. inline reference to \"Table 1\"\n",
    "                \"section\": \"Abstract\"\n",
    "            },\n",
    "            ...\n",
    "        ],\n",
    "        \"body_text\": [                      # list of paragraphs in full body\n",
    "                                            # paragraph dicts look the same as above\n",
    "            {\n",
    "                \"text\": <str>,\n",
    "                \"cite_spans\": [],\n",
    "                \"ref_spans\": [],\n",
    "                \"eq_spans\": [],\n",
    "                \"section\": \"Introduction\"\n",
    "            },\n",
    "            ...\n",
    "            {\n",
    "                ...,\n",
    "                \"section\": \"Conclusion\"\n",
    "            }\n",
    "        ],\n",
    "        \"bib_entries\": {\n",
    "            \"BIBREF0\": {\n",
    "                \"ref_id\": <str>,\n",
    "                \"title\": <str>,\n",
    "                \"authors\": <list of dict>       # same structure as earlier,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**预处理：**考虑只保留文献id、文献标题、文献摘要、文献全文和参考文献几部分。其中，若文献摘要缺失，采用赋值为空字符串处理；文献全文为文献正文分段对拼接；参考文献为参考文献标题的集合。考虑到巨大的数据量对于后续绘图速度的影响，在此只取10000份样本（即两个文件夹各读取5000份）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "names=[\"pmc_json\",\"pdf_json\"]\n",
    "docs=[]\n",
    "for d in names:\n",
    "    print(d)\n",
    "    counts = 0\n",
    "    for file in tqdm(os.listdir(f\"C:/Users/ASUS/Downloads/新建文件夹/archive/document_parses/{d}\")):\n",
    "        file_path = f\"C:/Users/ASUS/Downloads/新建文件夹/archive/document_parses/{d}/{file}\"\n",
    "        j = json.load(open(file_path,\"rb\"))\n",
    "        paper_id = j['paper_id']\n",
    "        paper_id = paper_id[-7:]\n",
    "        title = j['metadata']['title']\n",
    "\n",
    "        try:\n",
    "            abstract = j['abstract'][0]['text']  \n",
    "        except:\n",
    "            abstract = \"\" #sometimes there are no abstracts\n",
    "            \n",
    "        full_text = \"\"\n",
    "        bib_entries = []\n",
    "        for text in j['body_text']:\n",
    "            full_text += text['text']\n",
    "            for csp in text['cite_spans']:\n",
    "                try:\n",
    "                    title = j['bib_entries'][csp['ref_id']]['title']\n",
    "                    bib_entries.append(title)\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "        docs.append([paper_id, title, abstract, full_text, bib_entries])   \n",
    "        \n",
    "        #too many data lead to tremendous running time, thus only 10000 articles of each files are taken into accout\n",
    "        counts = counts + 1\n",
    "        if(counts > 5000): \n",
    "           break\n",
    "        \n",
    "df=pd.DataFrame(docs,columns=['paper_id','title','abstract','full_text','bib_entries']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经预处理，数据如下表所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 文献字数分布图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这一部分中，考虑分析文献的摘要和全文的字数分布，从而可以对文献的长度有一个基本的了解。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "def new_len(x):\n",
    "    if type(x) is str:\n",
    "        return len(x.split())\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "fig = make_subplots(rows=2, cols=1, shared_yaxes=False)\n",
    "df[\"abstract_num\"] = df[\"abstract\"].apply(new_len)\n",
    "nums1 = df.query(\"abstract_num != 0 and abstract_num < 500\")[\"abstract_num\"]\n",
    "fig = ff.create_distplot(hist_data=[nums1],\n",
    "                         group_labels=[\"All abstracts\"],\n",
    "                         colors=[\"coral\"])\n",
    "fig.update_layout(title_text=\"Abstract words\", xaxis_title=\"Abstract words\",  yaxis_range=[0,0.008],\n",
    "                  template=\"simple_white\", showlegend=False)\n",
    "fig.show()\n",
    "\n",
    "\n",
    "df[\"full_num\"] = df[\"full_text\"].apply(new_len)\n",
    "nums2 = df.query(\"full_num != 0 and full_num < 15000\")[\"full_num\"]\n",
    "fig = ff.create_distplot(hist_data=[nums2],\n",
    "                         group_labels=[\"All full texts\"],\n",
    "                         colors=[\"coral\"])\n",
    "fig.update_layout(title_text=\"Article words\", xaxis_title=\"Article words\", yaxis_range=[0,0.0003],\n",
    "                  template=\"simple_white\", showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**图表分析：**由上图所示，文章摘要的长度最多集中于155个词附近，而有点出人意料的是不少文献的摘要长度集中于40个词附近。总体来看，文献字数大致在300字以内，与我们认知中的文献字数大致相符，体现了摘要言简意赅的特点。\n",
    "文章的长度以900-3000字的分布最为集中，超过10000字的文献明显较少，从中可以看出文献大致以中短篇幅为主。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2  摘要词云图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "词云图可以用于展示文章中出现高频词，直观地反映文章涉及的大致内容。在此选取摘要做词云图是考虑到运算的速度，如果运用到全文也是大同小异的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word cloud for the abstracts\n",
    "from wordcloud import WordCloud\n",
    "ab=' '.join(list(df['abstract']))\n",
    "wordcloud=WordCloud(background_color=\"white\",width=1000,height=500,max_words=100).generate(ab)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**图表分析：**从上图可以看出，出现频率最高的是COVID和patient，除此之外SARS也是出现频率很高的词，可以想像这是因为SARS和新冠一样都是冠状病毒，有着一定的相似性。另外，有诸如method、model、result、treatment之类的高频词，这反映了不少文献涉及了模型或者方法，探讨了新冠的疗法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 国家出现频数分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**说明：**这一部分考虑分析文献正文中各国家被提及的次数，该分析可以看出在疫情中哪些国家较为令人瞩目，也可以从某些方面反映各国在疫情方面是否具有代表性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**分析过程：**\n",
    "首先对数据进行处理，通过pycountry模块可以获得国家的名称、代码等信息，为后续分析提供帮助。  \n",
    "然后定义word_count和get_target_dict函数用于从文本中寻找特定的国家名，返回字典形式，为之后的计数处理提供支持。  \n",
    "最后，计算各个国家在文献正文中出现的总次数，得到一个列表如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycountry\n",
    "def word_count(targets, df, text, key):\n",
    "    df_targets = df[df[text].apply(lambda sentence: any(word in sentence for word in targets))] \n",
    "    original = {}\n",
    "    word_distribution = {}\n",
    "    for index, row in df_targets.iterrows():\n",
    "        original, word_distribution = get_target_dict(targets, row[text], row[key], original, word_distribution)\n",
    "\n",
    "    return original, word_distribution\n",
    "\n",
    "import copy\n",
    "def get_target_dict(targets, text, paper_id, original, word_distribution):\n",
    "    word_count_new = {}\n",
    "    word_count = {}\n",
    "    for sentence in text.split('.'):\n",
    "        for word in targets:\n",
    "            if word in sentence:\n",
    "                if word in word_count: \n",
    "                    word_count[word] += sentence.count(word)\n",
    "                else:\n",
    "                    word_count[word] = sentence.count(word)\n",
    "        word_count_new = copy.deepcopy(word_count)\n",
    "        if bool(word_count_new):\n",
    "            for word in targets:\n",
    "                if word not in word_count_new:\n",
    "                    word_count_new[word] = 0\n",
    "    word_distribution[paper_id] = word_count_new\n",
    "    original[paper_id] = word_count \n",
    "    return original, word_distribution\n",
    "\n",
    "country_name=[]\n",
    "for country in pycountry.countries:\n",
    "    country_name.append(country.name)\n",
    "targets=country_name\n",
    "countries_mentioned,b = word_count(targets,df,'full_text','paper_id')\n",
    "total_count = {}\n",
    "for value in countries_mentioned.values():\n",
    "    total_count = {key: total_count.get(key, 0) + value.get(key, 0)\n",
    "          for key in set(total_count) | set(value)}\n",
    "for country,counts in total_count.items():\n",
    "    total_count[country]=[counts]\n",
    "print(\"{:<35} {:<7}\".format('Country','Total no. of times mentioned'))\n",
    "for k, v in total_count.items():\n",
    "     print(\"{:<35} {:<7}\".format(k, v[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "借助于上述数据，进行绘图。\n",
    "\n",
    "首先绘制地区分布图。该图是对世界地图的填色，具有一定的美观性。同时，地图清晰明了地将地理特点和词频高低结合起来，很好地体现了数据本身所具有的空间的概念。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "np.random.seed(12)\n",
    "gapminder = px.data.gapminder().query(\"year==2007\")[['country','continent','iso_alpha']]\n",
    "\n",
    "d = total_count\n",
    "\n",
    "data_country = pd.DataFrame(d).T.reset_index()\n",
    "data_country.columns=['country', 'count']\n",
    "\n",
    "df_merge=pd.merge(gapminder, data_country, on='country', how='left').dropna()\n",
    "\n",
    "fig = px.choropleth(df_merge, locations=\"iso_alpha\",\n",
    "                    color=\"count\", \n",
    "                    hover_name=\"country\",\n",
    "                    color_continuous_scale=px.colors.sequential.Emrld,\n",
    "                    projection='natural earth',\n",
    "                    title='Frequency of occurence of country')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**图表分析：**  \n",
    "从上图中，可以看到中国特别显眼，明显文献中中国被提及的次数原高于其它国家，这既有可能是因为疫情最初大面积爆发是在中国，也有可能因为中国在抗疫期间卓越的成效引人瞩目。  \n",
    "总体来看，亚洲、美洲、欧洲的颜色较深，这意味着这些大洲的国家往往被提及了更多次。这既有可能是因为大部分受瞩目的发达国家集中在这些洲，也很有可能是因为这些洲的不少国家疫情呈现井喷趋势。  \n",
    "其次，绘制旭日图。旭日图可以看作是升级版的饼图或是环形图，相比此二者，旭日图可以反映层级关系、从属关系。距离圆心较近者具有较高层级，距离圆心较远者具有较低层级。  \n",
    "在此，同时考虑国家与大洲。显然大洲是较高层级，在内侧。  \n",
    "下图可以非常直观地反映出被提及次数较多的国家的情况，可以反映出被提及次数较多的大洲的情况。  \n",
    "由于是交互图，点击某一个大洲名，还可以查看该大洲各国家被提及次数的环形图，非常直观清晰。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.sunburst(df_merge, path=['continent', 'country'], values='count',\n",
    "                  color='count', hover_data=['iso_alpha'],\n",
    "                  color_continuous_scale='YlOrBr',\n",
    "                  title='Frequency of occurence of country/continent')\n",
    "                  \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 其它疾病频数图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**说明：**  \n",
    "这一部分考虑新冠肺炎和其它一些常见疾病之间的关系，这里的关系是多种多样的，既可以是新冠会导致的疾病，也可以是某些可能更易导致新冠的疾病。而在文献研究中，不对上述关系作区分，所以只需考虑其出现的频数。  \n",
    "在此，考虑的常见疾病包括：哮喘、癌症、慢性病、糖尿病、痴呆、心脏病、高血压、呼吸道疾病。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diseases = ['asthma','cancer','chronic','diabetes','dementia','heart','hypertension','respiratory disease']\n",
    "countdis=[]\n",
    "for i in range(len(diseases)):\n",
    "    countdis.append(''.join(list(df['full_text'])).lower().count(diseases[i].lower()))\n",
    "diseases = ['Asthma','Cancer','Chronic Disease','Diabetes','Dementia','Heart Disease','Hypertension','Respiratory Disease']\n",
    "data_disease=pd.DataFrame({'diseases':diseases, 'number':countdis}).sort_values('number',ascending=False)\n",
    "data_disease.plot(x = 'diseases', \n",
    "    y = 'number', \n",
    "    kind='bar', \n",
    "    legend = False,\n",
    "    width=0.8)\n",
    "plt.xlabel(\"Diseases\")\n",
    "plt.ylabel(\"Number of occurence\")\n",
    "plt.title(\"Number of occurence of diseases\")\n",
    "plt.style.use('seaborn-white')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**图表分析：**\n",
    "上图反映在新冠文献中被提及最多的是癌症，其次是慢性病，再次是心脏病。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 热门参考文献"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**说明：**这一部分考察新冠参考文献所引用的文献，希望找到被引次数最多的一系列文献。  \n",
    "在此，考虑被引次数最多的10篇文献。  \n",
    "因为图片显示的原因，文献的名称有截断，但这基本不妨碍我们对于文献内容的大致了解。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "bibs=[]\n",
    "for item in df['bib_entries']:\n",
    "    for bib in item:\n",
    "        bibs.append(bib)\n",
    "a = dict(Counter(bibs))\n",
    "del a['']\n",
    "df_a=pd.DataFrame.from_dict(a, orient='index',columns=['no. of times cited'])\n",
    "df_a['no. of times cited'] = df_a['no. of times cited'].astype(str).astype(int)\n",
    "sorted_df_a=df_a.sort_values(by='no. of times cited', ascending=False)\n",
    "new_df = sorted_df_a.loc[sorted_df_a['no. of times cited'] >= 10] \n",
    "new_df['title'] = new_df.index.str.slice(0,50)\n",
    "\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-white')\n",
    "sns.set()  \n",
    "new_df[0:10].set_index('no. of times cited').reset_index().sort_index(ascending=False).plot(\n",
    "    x = 'title', \n",
    "    y = 'no. of times cited', \n",
    "    kind='barh', \n",
    "    legend = False,\n",
    "    width=0.8\n",
    ")\n",
    "plt.style.use('seaborn-white')\n",
    "plt.xlabel(\"Number of times cited\")\n",
    "plt.ylabel(\"Paper\")\n",
    "plt.title(\"Number of citations of top 10 papers\")\n",
    "plt.gca().yaxis.grid(linestyle='')\n",
    "plt.gca().xaxis.grid(linestyle=':')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**图表分析：**从上图可以看出，被引次数多的文献主要是关于新冠的临床症状。这反映了新冠作为一种新的传染病，学界对于其的了解处在一个相对比较初级的阶段，所以其症状会被关心。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**章节小结：**综上，这一部分大致对目前的与新冠相关的文献进行了一个初步的分析，可以从中初步了解文献的特点和文献研究的热点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"id_prediction\"></a>\n",
    "# 3. COVID-19各国确诊人数的预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**章节说明：**在了解到当今疫情的现状以及知道了如今研究新冠肺炎的文献都有哪些特点之后，我们或许可以做的更多，去分析未来各个国家的确诊人数的发展方向，这些国家的确诊病例是否会在长期内继续增长，增长的速率又是如何，又会在哪一时期趋于稳定，接下来就通过这个预测模型来进行说明。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 长期预测：Sigmoid Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基本的思路是用sigmoid函数来拟合每个国家的确诊病例数（sigmoid函数呈现S型，平滑、易于求导，常用于激活函数）。从病毒的传播模型来看，我们采用了sigmoid曲线去拟合数据是比较自然的，它也是很多研究者所采用的的方法之一。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top30_countries = top_country_df.sort_values('confirmed', ascending=False).iloc[:30]['country'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t, M, beta, alpha, offset=0):\n",
    "    alpha += offset\n",
    "    return M / (1 + np.exp(-beta * (t - alpha)))\n",
    "\n",
    "def error(x, y, params):\n",
    "    M, beta, alpha = params\n",
    "    y_pred = sigmoid(x, M, beta, alpha)\n",
    "\n",
    "    # apply weight, latest number is more important than past.\n",
    "    weight = np.arange(len(y_pred)) ** 2\n",
    "    loss_mse = np.mean((y_pred - y) ** 2 * weight)\n",
    "    return loss_mse\n",
    "\n",
    "def gen_random_color(min_value=0, max_value=256) -> str:\n",
    "    \"\"\"Generate random color for plotly\"\"\"\n",
    "    r, g, b = np.random.randint(min_value, max_value, 3)\n",
    "    return f'rgb({r},{g},{b})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_sigmoid(exclude_days=0):\n",
    "    target_country_df_list = []\n",
    "    pred_df_list = []\n",
    "    for target_country in top30_countries:\n",
    "        print('target_country', target_country)\n",
    "        # --- Train ---\n",
    "        target_country_df = country_df.query('country == @target_country')\n",
    "\n",
    "        #train_start_date = target_country_df['date'].min()\n",
    "        train_start_date = target_country_df.query('confirmed > 1000')['date'].min()\n",
    "        train_end_date = pd.to_datetime(target_date) - pd.Timedelta(f'{exclude_days} days')\n",
    "        target_date_df = target_country_df.query('(date >= @train_start_date) & (date <= @train_end_date)')\n",
    "        if len(target_date_df) <= 7:\n",
    "            print('WARNING: the data is not enough, use 7 more days...')\n",
    "            train_start_date -= pd.Timedelta('7 days')\n",
    "            target_date_df = target_country_df.query('(date >= @train_start_date) & (date <= @train_end_date)')\n",
    "\n",
    "        confirmed = target_date_df['confirmed'].values\n",
    "        x = np.arange(len(confirmed))\n",
    "\n",
    "        lossfun = lambda params: error(x, confirmed, params)\n",
    "        res = sp.optimize.minimize(lossfun, x0=[np.max(confirmed) * 5, 0.04, 2 * len(confirmed) / 3.], method='nelder-mead')\n",
    "        M, beta, alpha = res.x\n",
    "        # sigmoid_models[key] = (M, beta, alpha)\n",
    "        # np.clip(sigmoid(list(range(len(data), len(data) + steps)), M, beta, alpha), 0, None).astype(int)\n",
    "\n",
    "        # --- Pred ---\n",
    "        pred_start_date = target_country_df['date'].min()\n",
    "        pred_end_date = pd.to_datetime('2021-06-01')\n",
    "        days = int((pred_end_date - pred_start_date) / pd.Timedelta('1 days'))\n",
    "        # print('pred start', pred_start_date, 'end', pred_end_date, 'days', days)\n",
    "\n",
    "        x = np.arange(days)\n",
    "        offset = (train_start_date - pred_start_date) / pd.Timedelta('1 days')\n",
    "        print('train_start_date', train_start_date, 'offset', offset, 'params', M, beta, alpha)\n",
    "        y_pred = sigmoid(x, M, beta, alpha, offset=offset)\n",
    "        # target_country_df['confirmed_pred'] = y_pred\n",
    "\n",
    "        all_dates = [pred_start_date + np.timedelta64(x, 'D') for x in range(days)]\n",
    "        pred_df = pd.DataFrame({\n",
    "            'date': all_dates,\n",
    "            'country': target_country,\n",
    "            'confirmed_pred': y_pred,\n",
    "        })\n",
    "\n",
    "        target_country_df_list.append(target_country_df)\n",
    "        pred_df_list.append(pred_df)\n",
    "    return target_country_df_list, pred_df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sigmoid_fitting(target_country_df_list, pred_df_list, title=''):\n",
    "    n_countries = len(top30_countries)\n",
    "\n",
    "    # --- Plot ---\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for i in range(n_countries):\n",
    "        target_country = top30_countries[i]\n",
    "        target_country_df = target_country_df_list[i]\n",
    "        pred_df = pred_df_list[i]\n",
    "        color = gen_random_color(min_value=20)\n",
    "        # Prediction\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=pred_df['date'], y=pred_df['confirmed_pred'],\n",
    "            name=f'{target_country}_pred',\n",
    "            line=dict(color=color, dash='dash')\n",
    "        ))\n",
    "\n",
    "        # Ground truth\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=target_country_df['date'], y=target_country_df['confirmed'],\n",
    "            mode='markers', name=f'{target_country}_actual',\n",
    "            line=dict(color=color),\n",
    "        ))\n",
    "    fig.update_layout(\n",
    "        title=title, xaxis_title='Date', yaxis_title='Confirmed cases')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_country_df_list, pred_df_list = fit_sigmoid(exclude_days=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过sigmoid函数我们能得到每个国家确诊数的预测值，我们的预测期数是144期，到2021-06-01。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sigmoid_fitting(target_country_df_list, pred_df_list, title='Sigmoid fitting with all latest data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**图表分析：**绘制当前实际值与未来144期预测值的时间序列折线图如下，可以看到确诊病例正在放缓，而且全球大部分地区相同。就目前情势来看，如果美国和巴西没有根本性政策改变的话，确诊病例到6月初依旧会有增加。因此，要在全球范围内控制住疫情，还需要长期的努力。我们也面临着第二波挑战，也就是春节的人口大规模流动。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_country_df_list, pred_df_list = fit_sigmoid(exclude_days=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于数据量太大，模型的检验部分我们才去直接采取带入实际值进行效果判定，具体方法是排除过去7天的数据来再次拟合模型进行判定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sigmoid_fitting(target_country_df_list, pred_df_list, title='Sigmoid fitting without last 7days data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**图表分析：**从图上我们可以注意到sigmoid拟合往往低估了曲线，确诊病例实际数值往往比sigmoid曲线估计值要大。因此，sigmoid曲线预测数据数据往往更加保守，实际情况很可能比之前用所有数据训练出的模型预测值更高，我们应该考虑更高的风险值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 短期预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**说明：**对模型预测期数的讨论，长期期数下，预测效果一定存在很大的偏误，应该尽可能避免长期预测，而是采用短期预测，等待新样本加入模型之后再进行短期预测，才会有较高的预测精度。通过查找文献，我们尝试实现了几种方法来实现短期预测，有支持向量机预测，多项式函数预测，贝叶斯回归这三种方法，只用短期数据做了误差分析，得到的结论是支持向量机在处理短期确诊人数的预测时表现较为优良。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 SVM预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.colors as mcolors\n",
    "import pandas as pd \n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import datetime\n",
    "import operator \n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('retina')\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirmed_df = pd.read_csv('time_series_covid19_confirmed_global.csv')\n",
    "deaths_df = pd.read_csv('time_series_covid19_deaths_global.csv')\n",
    "recoveries_df = pd.read_csv('time_series_covid19_recovered_global.csv')\n",
    "latest_data = pd.read_csv('01-07-2021.csv')\n",
    "us_medical_data = pd.read_csv('01-07-2021(2).csv')\n",
    "\n",
    "cols = confirmed_df.keys()\n",
    "\n",
    "confirmed = confirmed_df.loc[:, cols[4]:cols[-1]]\n",
    "deaths = deaths_df.loc[:, cols[4]:cols[-1]]\n",
    "recoveries = recoveries_df.loc[:, cols[4]:cols[-1]]\n",
    "\n",
    "dates = confirmed.keys()\n",
    "world_cases = []\n",
    "total_deaths = [] \n",
    "mortality_rate = []\n",
    "recovery_rate = [] \n",
    "total_recovered = [] \n",
    "total_active = [] \n",
    "\n",
    "for i in dates:\n",
    "    confirmed_sum = confirmed[i].sum()\n",
    "    death_sum = deaths[i].sum()\n",
    "    recovered_sum = recoveries[i].sum()\n",
    "    \n",
    "    # confirmed, deaths, recovered, and active\n",
    "    world_cases.append(confirmed_sum)\n",
    "    total_deaths.append(death_sum)\n",
    "    total_recovered.append(recovered_sum)\n",
    "    total_active.append(confirmed_sum-death_sum-recovered_sum)\n",
    "    \n",
    "    # calculate rates\n",
    "    mortality_rate.append(death_sum/confirmed_sum)\n",
    "    recovery_rate.append(recovered_sum/confirmed_sum)\n",
    "    \n",
    "    \n",
    "def daily_increase(data):\n",
    "    d = [] \n",
    "    for i in range(len(data)):\n",
    "        if i == 0:\n",
    "            d.append(data[0])\n",
    "        else:\n",
    "            d.append(data[i]-data[i-1])\n",
    "    return d \n",
    "\n",
    "def moving_average(data, window_size):\n",
    "    moving_average = []\n",
    "    for i in range(len(data)):\n",
    "        if i + window_size < len(data):\n",
    "            moving_average.append(np.mean(data[i:i+window_size]))\n",
    "        else:\n",
    "            moving_average.append(np.mean(data[i:len(data)]))\n",
    "    return moving_average\n",
    "\n",
    "# window size\n",
    "window = 7\n",
    "\n",
    "# confirmed cases\n",
    "world_daily_increase = daily_increase(world_cases)\n",
    "world_confirmed_avg= moving_average(world_cases, window)\n",
    "world_daily_increase_avg = moving_average(world_daily_increase, window)\n",
    "\n",
    "# deaths\n",
    "world_daily_death = daily_increase(total_deaths)\n",
    "world_death_avg = moving_average(total_deaths, window)\n",
    "world_daily_death_avg = moving_average(world_daily_death, window)\n",
    "\n",
    "\n",
    "# recoveries\n",
    "world_daily_recovery = daily_increase(total_recovered)\n",
    "world_recovery_avg = moving_average(total_recovered, window)\n",
    "world_daily_recovery_avg = moving_average(world_daily_recovery, window)\n",
    "\n",
    "\n",
    "# active \n",
    "world_active_avg = moving_average(total_active, window)\n",
    "\n",
    "days_since_1_22 = np.array([i for i in range(len(dates))]).reshape(-1, 1)\n",
    "world_cases = np.array(world_cases).reshape(-1, 1)\n",
    "total_deaths = np.array(total_deaths).reshape(-1, 1)\n",
    "total_recovered = np.array(total_recovered).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_in_future = 10\n",
    "future_forcast = np.array([i for i in range(len(dates)+days_in_future)]).reshape(-1, 1)\n",
    "adjusted_dates = future_forcast[:-10]\n",
    "\n",
    "start = '1/22/2020'\n",
    "start_date = datetime.datetime.strptime(start, '%m/%d/%Y')\n",
    "future_forcast_dates = []\n",
    "for i in range(len(future_forcast)):\n",
    "    future_forcast_dates.append((start_date + datetime.timedelta(days=i)).strftime('%m/%d/%Y'))\n",
    "    \n",
    "# slightly modify the data to fit the model better (regression models cannot pick the pattern)\n",
    "X_train_confirmed, X_test_confirmed, y_train_confirmed, y_test_confirmed = train_test_split(days_since_1_22[50:], world_cases[50:], test_size=0.05, shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm_confirmed = svm_search.best_estimator_\n",
    "svm_confirmed = SVR(shrinking=True, kernel='poly',gamma=0.01, epsilon=1,degree=3, C=0.1)\n",
    "svm_confirmed.fit(X_train_confirmed, y_train_confirmed)\n",
    "svm_pred = svm_confirmed.predict(future_forcast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check against testing data\n",
    "svm_test_pred = svm_confirmed.predict(X_test_confirmed)\n",
    "plt.plot(y_test_confirmed)\n",
    "plt.plot(svm_test_pred)\n",
    "plt.legend(['Test Data', 'SVM Predictions'])\n",
    "print('MAE:', mean_absolute_error(svm_test_pred, y_test_confirmed))\n",
    "print('MSE:',mean_squared_error(svm_test_pred, y_test_confirmed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform our data for polynomial regression\n",
    "poly = PolynomialFeatures(degree=4)\n",
    "poly_X_train_confirmed = poly.fit_transform(X_train_confirmed)\n",
    "poly_X_test_confirmed = poly.fit_transform(X_test_confirmed)\n",
    "poly_future_forcast = poly.fit_transform(future_forcast)\n",
    "\n",
    "bayesian_poly = PolynomialFeatures(degree=5)\n",
    "bayesian_poly_X_train_confirmed = bayesian_poly.fit_transform(X_train_confirmed)\n",
    "bayesian_poly_X_test_confirmed = bayesian_poly.fit_transform(X_test_confirmed)\n",
    "bayesian_poly_future_forcast = bayesian_poly.fit_transform(future_forcast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# polynomial regression\n",
    "linear_model = LinearRegression(normalize=True, fit_intercept=False)\n",
    "linear_model.fit(poly_X_train_confirmed, y_train_confirmed)\n",
    "test_linear_pred = linear_model.predict(poly_X_test_confirmed)\n",
    "linear_pred = linear_model.predict(poly_future_forcast)\n",
    "print('MAE:', mean_absolute_error(test_linear_pred, y_test_confirmed))\n",
    "print('MSE:',mean_squared_error(test_linear_pred, y_test_confirmed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(linear_model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_test_confirmed)\n",
    "plt.plot(test_linear_pred)\n",
    "plt.legend(['Test Data', 'Polynomial Regression Predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Bayesian Ridge Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bayesian ridge polynomial regression\n",
    "tol = [1e-6, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "alpha_1 = [1e-7, 1e-6, 1e-5, 1e-4, 1e-3]\n",
    "alpha_2 = [1e-7, 1e-6, 1e-5, 1e-4, 1e-3]\n",
    "lambda_1 = [1e-7, 1e-6, 1e-5, 1e-4, 1e-3]\n",
    "lambda_2 = [1e-7, 1e-6, 1e-5, 1e-4, 1e-3]\n",
    "normalize = [True, False]\n",
    "\n",
    "bayesian_grid = {'tol': tol, 'alpha_1': alpha_1, 'alpha_2' : alpha_2, 'lambda_1': lambda_1, 'lambda_2' : lambda_2, \n",
    "                 'normalize' : normalize}\n",
    "\n",
    "bayesian = BayesianRidge(fit_intercept=False)\n",
    "bayesian_search = RandomizedSearchCV(bayesian, bayesian_grid, scoring='neg_mean_squared_error', cv=3, return_train_score=True, n_jobs=-1, n_iter=40, verbose=1)\n",
    "bayesian_search.fit(bayesian_poly_X_train_confirmed, y_train_confirmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesian_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesian_confirmed = bayesian_search.best_estimator_\n",
    "test_bayesian_pred = bayesian_confirmed.predict(bayesian_poly_X_test_confirmed)\n",
    "bayesian_pred = bayesian_confirmed.predict(bayesian_poly_future_forcast)\n",
    "print('MAE:', mean_absolute_error(test_bayesian_pred, y_test_confirmed))\n",
    "print('MSE:',mean_squared_error(test_bayesian_pred, y_test_confirmed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_test_confirmed)\n",
    "plt.plot(test_bayesian_pred)\n",
    "plt.legend(['Test Data', 'Bayesian Ridge Polynomial Predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用短期数据做了误差分析，得到的结论是支持向量机在处理短期确诊人数的预测时表现较为优良。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**章节小结：**  \n",
    "1.对于确诊人数的预测我们还存在很多可以改进的地方。比如对模型的稳健性和灵敏度的讨论，考虑不同国家未来抗疫政策对模型的影响，考虑例如春节这种会对疫情带来显著影响的强影响事件。  \n",
    "2.通过查找文献，我们还了解到其他类别的方法，比如利用社会学中信息传播模型来类比建立关于新冠肺炎的传播模型，因为每个感染者/创新采纳者的接触人数似乎是相关的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"id_medicine\"></a>\n",
    "# 4. 生物统计行业"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 生物统计相关技能要求"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#代码部分需要记得更改文件路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud,STOPWORDS,ImageColorGenerator\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from imageio import imread\n",
    "import random\n",
    "\n",
    "d = path.dirname(__file__) if \"__file__\" in locals() else os.getcwd()\n",
    "    # 获取文本text\n",
    "#text = open(path.join(d,'JD信息.txt')).read()\n",
    "text=open('/Users/haoranzhang/人大ISBD/数据可视化/数据可视化final project+董瑶_刘昀霖_张浩然/Data/JD信息.txt').read()\n",
    "    # 读取背景图片\n",
    "background_Image = imread('/Users/haoranzhang/人大ISBD/数据可视化/数据可视化final project+董瑶_刘昀霖_张浩然/Data/可视化.png')\n",
    "    # 提取背景图片颜色\n",
    "img_colors = ImageColorGenerator(background_Image)\n",
    "    # 设置英文停止词\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.add('ability')\n",
    "stopwords.add('able')\n",
    "stopwords.add('interests')\n",
    "stopwords.add('address')\n",
    "stopwords.add('carries')\n",
    "stopwords.add('explanation')\n",
    "stopwords.add('generalization')\n",
    "stopwords.add('preparation')\n",
    "stopwords.add('abstracts')\n",
    "stopwords.add('duties')\n",
    "stopwords.add('work')\n",
    "#print(stopwords)\n",
    "\n",
    "\n",
    "wc = WordCloud(\n",
    "    width=1000,\n",
    "    height=700,\n",
    "    margin = 2, # 设置页面边缘\n",
    "    mask = background_Image,\n",
    "    scale = 30,\n",
    "    max_words = 20, # 最多词个数\n",
    "    min_font_size = 4, # 最小字体大小\n",
    "    stopwords = stopwords,\n",
    "    random_state = 42,\n",
    "    background_color = 'white', # 背景颜色\n",
    "    max_font_size = 150, # 最大字体大小\n",
    "    colormap = 'RdYlGn'\n",
    "    )\n",
    "    # 生成词云\n",
    "wc.generate_from_text(text)\n",
    "    # 等价于\n",
    "    # wc.generate(text)\n",
    "    # 根据图片色设置背景色\n",
    "#wc.recolor(color_func=img_colors)\n",
    "    #存储图像\n",
    "wc.to_file('JD信息词云图.png')\n",
    "    # 显示图像\n",
    "\n",
    "plt.imshow(wc,interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了统计软件之外，出版物、设备操作、热情也是求职生物统计岗位的重要因素。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 生物统计相关岗位"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud,STOPWORDS,ImageColorGenerator\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from imageio import imread\n",
    "import random\n",
    "\n",
    "d = path.dirname(__file__) if \"__file__\" in locals() else os.getcwd()\n",
    "    # 获取文本text\n",
    "#text = open(path.join(d,'岗位名称.txt')).read()\n",
    "text=open('/Users/haoranzhang/人大ISBD/数据可视化/数据可视化final project+董瑶_刘昀霖_张浩然/Data/岗位名称.txt').read()\n",
    "    # 读取背景图片\n",
    "background_Image = imread('/Users/haoranzhang/人大ISBD/数据可视化/数据可视化final project+董瑶_刘昀霖_张浩然/Data/可视化.png')\n",
    "    # 提取背景图片颜色\n",
    "img_colors = ImageColorGenerator(background_Image)\n",
    "    # 设置英文停止词\n",
    "stopwords = set(STOPWORDS)\n",
    "#print(stopwords)\n",
    "\n",
    "\n",
    "wc = WordCloud(\n",
    "    width=1000,\n",
    "    height=700,\n",
    "    margin = 2, # 设置页面边缘\n",
    "    mask = background_Image,\n",
    "    scale = 30,\n",
    "    max_words = 20, # 最多词个数\n",
    "    min_font_size = 4, # 最小字体大小\n",
    "    stopwords = stopwords,\n",
    "    random_state = 42,\n",
    "    background_color = 'white', # 背景颜色\n",
    "    max_font_size = 150, # 最大字体大小\n",
    "    colormap = 'winter'\n",
    "    )\n",
    "    # 生成词云\n",
    "wc.generate_from_text(text)\n",
    "    # 等价于\n",
    "    # wc.generate(text)\n",
    "    # 根据图片色设置背景色\n",
    "#wc.recolor(color_func=img_colors)\n",
    "    #存储图像\n",
    "wc.to_file('岗位名称词云图.png')\n",
    "    # 显示图像\n",
    "\n",
    "plt.imshow(wc,interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生物统计相关工作不只局限在生物统计，数据科学家、生物信息学家都有一席之地。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 生物统计相关公司"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud,STOPWORDS,ImageColorGenerator\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from imageio import imread\n",
    "import random\n",
    "\n",
    "d = path.dirname(__file__) if \"__file__\" in locals() else os.getcwd()\n",
    "    # 获取文本text\n",
    "#text = open(path.join(d,'公司名称.txt')).read()\n",
    "text=open('/Users/haoranzhang/人大ISBD/数据可视化/数据可视化final project+董瑶_刘昀霖_张浩然/Data/公司名称.txt').read()\n",
    "    # 读取背景图片\n",
    "background_Image = imread('/Users/haoranzhang/人大ISBD/数据可视化/数据可视化final project+董瑶_刘昀霖_张浩然/Data/可视化.png')\n",
    "    # 提取背景图片颜色\n",
    "img_colors = ImageColorGenerator(background_Image)\n",
    "    # 设置英文停止词\n",
    "stopwords = set(STOPWORDS)\n",
    "#print(stopwords)\n",
    "\n",
    "\n",
    "wc = WordCloud(\n",
    "    width=1000,\n",
    "    height=700,\n",
    "    margin = 2, # 设置页面边缘\n",
    "    mask = background_Image,\n",
    "    scale = 30,\n",
    "    max_words = 20, # 最多词个数\n",
    "    min_font_size = 4, # 最小字体大小\n",
    "    stopwords = stopwords,\n",
    "    random_state = 42,\n",
    "    background_color = 'white', # 背景颜色\n",
    "    max_font_size = 150, # 最大字体大小\n",
    "    colormap = 'viridis'\n",
    "    )\n",
    "    # 生成词云\n",
    "wc.generate_from_text(text)\n",
    "    # 等价于\n",
    "    # wc.generate(text)\n",
    "    # 根据图片色设置背景色\n",
    "#wc.recolor(color_func=img_colors)\n",
    "    #存储图像\n",
    "wc.to_file('公司名称词云图.png')\n",
    "    # 显示图像\n",
    "\n",
    "plt.imshow(wc,interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大型外企药厂是招收生物统计职位最多的公司，例如拜耳等。高校实验室和医院也有一定的需求。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
